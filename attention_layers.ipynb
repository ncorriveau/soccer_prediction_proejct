{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNo+peOq3+pjQyRz1SnUu5c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ncorriveau/transformers_for_prediction/blob/main/attention_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPtMTp3TXrBp",
        "outputId": "d373c2fc-bbdb-4bcc-c794-7946d1a49165"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "EIlpu7lkXzJF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load our numpy tensors to load into keras \n",
        "features_path = '/content/drive/MyDrive/Data Mining/features.npy'\n",
        "labels_path = '/content/drive/MyDrive/Data Mining/labels.npy'\n",
        "features = np.load(features_path)\n",
        "labels = np.load(labels_path)"
      ],
      "metadata": {
        "id": "0chvsS9zYQ5R"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(features.shape)\n",
        "print(labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyzQOW2gYpcE",
        "outputId": "9aed3181-5a34-45a2-a337-702bb45282f1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(21361, 22, 66)\n",
            "(21361, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize the numerical features which are locatead in cols 0-35\n",
        "features_num = features[:,:,:35]\n",
        "print(features_num.shape)\n",
        "\n",
        "features_num[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqWIojDFZeTE",
        "outputId": "98f37b95-3992-43fa-bf9c-a79a137c0a62"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(21361, 22, 35)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([58., 67., 25., 25., 25., 25., 10., 25., 16., 12., 46., 35., 45.,\n",
              "       41., 41., 57., 72., 25., 56., 40., 73., 25., 57., 40., 33., 50.,\n",
              "       48., 25., 25., 35., 63., 51., 46., 53., 68.])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = np.mean(features_num,axis=(0,1))\n",
        "print(f\"shape of mean vector = {mean.shape}\")\n",
        "\n",
        "std = np.std(features_num,axis=(0,1))\n",
        "print(f\"shape of std vector = {std.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWKPDTLBb_xe",
        "outputId": "c0e4ac40-b841-41f9-ea34-abd6a8aaea26"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of mean vector = (35,)\n",
            "shape of std vector = (35,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_num = (features_num - mean) / std\n",
        "print(features_num.shape)\n",
        "print(features_num[0][0])\n",
        "\n",
        "features[:,:,:35] = features_num \n",
        "print(features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Uiykt1Le0Vb",
        "outputId": "10988194-f5ce-429b-b8e4-4286e44d1e55"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(21361, 22, 35)\n",
            "[-2.13402071 -1.37252412 -1.77469422 -1.23838184 -1.99655923 -2.73920252\n",
            " -2.04805005 -1.86919281 -1.97499507 -2.01346997 -0.99274355 -1.92213186\n",
            " -1.8076617  -2.24156397 -2.00765514 -1.48199584  0.46714265 -2.30508048\n",
            " -1.20626801 -2.30813745  0.28191452 -1.50510393 -0.48443304 -0.86434178\n",
            " -1.24425375 -0.70543806 -0.52144225 -1.16782289 -1.33347824 -0.76945225\n",
            "  2.46779935  1.85987951  0.88080786  1.93442016  2.57236977]\n",
            "(21361, 22, 66)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_num = np.round(features.shape[0]*0.7)\n",
        "val_num = np.round(features.shape[0]*0.10)\n",
        "test_num = np.round(features.shape[0]*0.2)\n",
        "train_index = int(train_num)\n",
        "val_index = int(train_num+val_num)\n",
        "test_index = int(val_index+test_num)\n",
        "\n",
        "print(f\"Training sample size = {train_num}, Validation set size = {val_num}, Test set size = {test_num}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj0zzDEXYudr",
        "outputId": "e6645d99-49eb-4db3-def0-f2ee7d33323e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training sample size = 14953.0, Validation set size = 2136.0, Test set size = 4272.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are working with time series data, i.e. each in the match dataset and thus our numpy array is in ordered by match date. "
      ],
      "metadata": {
        "id": "q3pALgFpl9k2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features = features[:train_index,:,:]\n",
        "train_labels = labels[:train_index,:]\n",
        "\n",
        "val_features = features[train_index:val_index,:,:]\n",
        "val_labels = labels[train_index:val_index,:]\n",
        "\n",
        "test_features = features[val_index:,:,:]\n",
        "test_labels = labels[val_index:,:]"
      ],
      "metadata": {
        "id": "-195LDKtblLi"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_features.shape, test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naQ1hbtbhBb2",
        "outputId": "4d2828c4-aa92-4004-bcf0-2d2697df93fa"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4272, 22, 66) (4272, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#turn data into dataset objects \n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_features, val_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels))"
      ],
      "metadata": {
        "id": "gXwXSMFMkk3e"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, now we finally have data that's ready to go in tf. let's run it through a very simple network and see what we get. "
      ],
      "metadata": {
        "id": "grU63LpDlrhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_dataset.batch(32).shuffle(100)\n",
        "\n",
        "simple_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(3,activation=\"softmax\",)\n",
        "])\n",
        "\n",
        "simple_model.compile(optimizer='adam',\n",
        "              loss=['categorical_crossentropy'] ,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "simple_model.build(input_shape=(,22,66))\n",
        "simple_model.summary()"
      ],
      "metadata": {
        "id": "4j0wWwaJlHcw"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_model.fit(train_ds, epochs=10)"
      ],
      "metadata": {
        "id": "bSYCK2fdrU6V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "803cd6db-251b-4609-988b-81b3736ffff6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "468/468 [==============================] - 2s 2ms/step - loss: 1.0736 - accuracy: 0.4741\n",
            "Epoch 2/10\n",
            "468/468 [==============================] - 1s 2ms/step - loss: 1.0319 - accuracy: 0.4978\n",
            "Epoch 3/10\n",
            "468/468 [==============================] - 1s 2ms/step - loss: 1.0135 - accuracy: 0.5085\n",
            "Epoch 4/10\n",
            "468/468 [==============================] - 1s 2ms/step - loss: 1.0087 - accuracy: 0.5133\n",
            "Epoch 5/10\n",
            "468/468 [==============================] - 1s 2ms/step - loss: 0.9899 - accuracy: 0.5205\n",
            "Epoch 6/10\n",
            "468/468 [==============================] - 1s 2ms/step - loss: 0.9960 - accuracy: 0.5217\n",
            "Epoch 7/10\n",
            "468/468 [==============================] - 1s 2ms/step - loss: 0.9903 - accuracy: 0.5285\n",
            "Epoch 8/10\n",
            "468/468 [==============================] - 1s 2ms/step - loss: 0.9810 - accuracy: 0.5282\n",
            "Epoch 9/10\n",
            "468/468 [==============================] - 1s 2ms/step - loss: 0.9901 - accuracy: 0.5263\n",
            "Epoch 10/10\n",
            "468/468 [==============================] - 1s 2ms/step - loss: 0.9826 - accuracy: 0.5316\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1154ab5a10>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we basically fed the data into a random model, and it spit out a train accuracy of ~53% which is ok. So this is a base case structure that we can build off of. Now we will implement a transformer architecture. starting with a linaer projection layer, followed by a vanilla transformer encoder, and lastly a multiclass classification head to output our 3 part vector of [home win, home tie, home loss] probabilities"
      ],
      "metadata": {
        "id": "xN_bFd6juCKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "\n",
        "#create the transformer encoder \n",
        "class TransformerEncoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "      super().__init__(**kwargs)\n",
        "      self.embed_dim = embed_dim\n",
        "      self.dense_dim = dense_dim\n",
        "      self.num_heads = num_heads\n",
        "      self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "      self.dense_proj = keras.Sequential(\n",
        "          [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "          layers.Dense(embed_dim),]\n",
        "          )\n",
        "      self.layernorm_1 = layers.LayerNormalization()\n",
        "      self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "      attention_output = self.attention(\n",
        "                inputs, inputs, inputs)\n",
        "      proj_input = self.layernorm_1(inputs + attention_output)\n",
        "      proj_output = self.dense_proj(proj_input)\n",
        "      return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "  def get_config(self):\n",
        "      config = super().get_config()\n",
        "      config.update({\n",
        "      \"embed_dim\": self.embed_dim,\n",
        "      \"num_heads\": self.num_heads,\n",
        "      \"dense_dim\": self.dense_dim,\n",
        "      })\n",
        "      return config\n"
      ],
      "metadata": {
        "id": "nWuiwaS5uraS"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create the full flow \n",
        "embed_dim = 256 \n",
        "num_heads = 4\n",
        "dense_dim = 32\n",
        "input_shape = (features.shape[1],features.shape[-1])\n",
        "print(input_shape)\n",
        "\n",
        "#create full flow of model \n",
        "inputs = keras.Input(shape=(input_shape), dtype=\"int64\")\n",
        "x = layers.Dense(embed_dim, activation=\"relu\")(inputs)\n",
        "#run through self encoding 12 times \n",
        "x = keras.Sequential([ \n",
        "    TransformerEncoder(embed_dim, dense_dim, num_heads),\n",
        "    TransformerEncoder(embed_dim, dense_dim, num_heads),\n",
        "    TransformerEncoder(embed_dim, dense_dim, num_heads),\n",
        "    TransformerEncoder(embed_dim, dense_dim, num_heads),\n",
        "    TransformerEncoder(embed_dim, dense_dim, num_heads),\n",
        "    TransformerEncoder(embed_dim, dense_dim, num_heads),\n",
        "    TransformerEncoder(embed_dim, dense_dim, num_heads),\n",
        "    TransformerEncoder(embed_dim, dense_dim, num_heads),\n",
        "    TransformerEncoder(embed_dim, dense_dim, num_heads),\n",
        "    TransformerEncoder(embed_dim, dense_dim, num_heads),\n",
        "    TransformerEncoder(embed_dim, dense_dim, num_heads),\n",
        "    TransformerEncoder(embed_dim, dense_dim, num_heads),  \n",
        "  ])(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "outputs = layers.Dense(3, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer='adam',\n",
        "              loss=['categorical_crossentropy'] ,\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5LtiQBS4uXL",
        "outputId": "0f704e51-7181-48de-f67c-033261d2c130"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(22, 66)\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 22, 66)]          0         \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 22, 256)           17152     \n",
            "                                                                 \n",
            " sequential_43 (Sequential)  (None, 22, 256)           12835200  \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 256)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_81 (Dense)            (None, 3)                 771       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,853,123\n",
            "Trainable params: 12,853,123\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_dataset.shuffle(100).batch(32)\n",
        "val_ds = val_dataset.batch(32)\n",
        "test_ds = test_dataset.batch(32)\n",
        "\n",
        "callbacks = keras.callbacks.ModelCheckpoint(filepath=\"transformer_encoder.keras\",\n",
        " monitor=\"val_loss\",\n",
        " save_best_only=True,\n",
        " )\n",
        "\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=10,\n",
        " callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\n",
        " \"transformer_encoder.keras\",\n",
        " custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
        "print(f\"Test acc: {model.evaluate(test_ds):.3f}\")\n"
      ],
      "metadata": {
        "id": "9TqHNLlA8xsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jrbhXdKNHQlK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}