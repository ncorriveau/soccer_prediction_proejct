{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPMMJGLLPYN0UlB2KePay18",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ncorriveau/transformers_for_prediction/blob/main/attention_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Processing \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ziA0RLzcuFEn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPtMTp3TXrBp",
        "outputId": "6463679d-2ab8-4fe3-dbde-74caad6852d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4pQ362TX_w4",
        "outputId": "65ed0991-6bef-4508-bb0b-1a0ad3eb0245"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 31.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "from keras import layers\n",
        "import tensorflow_addons as tfa"
      ],
      "metadata": {
        "id": "EIlpu7lkXzJF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load our numpy tensors to load into keras \n",
        "features_path = '/content/drive/MyDrive/Data Mining/features.npy'\n",
        "labels_path = '/content/drive/MyDrive/Data Mining/labels.npy'\n",
        "features = np.load(features_path)\n",
        "labels = np.load(labels_path)"
      ],
      "metadata": {
        "id": "0chvsS9zYQ5R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(features.shape)\n",
        "print(labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyzQOW2gYpcE",
        "outputId": "cc4265cc-ff04-44c5-b9bd-052143782ca4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(21361, 22, 66)\n",
            "(21361, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize the numerical features which are locatead in cols 0-35\n",
        "features_num = features[:,:,:35]\n",
        "print(features_num.shape)\n",
        "\n",
        "features_num[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqWIojDFZeTE",
        "outputId": "65ec2e18-e0f6-4602-d4d1-c39cd96298ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(21361, 22, 35)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([58., 67., 25., 25., 25., 25., 10., 25., 16., 12., 46., 35., 45.,\n",
              "       41., 41., 57., 72., 25., 56., 40., 73., 25., 57., 40., 33., 50.,\n",
              "       48., 25., 25., 35., 63., 51., 46., 53., 68.])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = np.mean(features_num,axis=(0,1))\n",
        "print(f\"shape of mean vector = {mean.shape}\")\n",
        "\n",
        "std = np.std(features_num,axis=(0,1))\n",
        "print(f\"shape of std vector = {std.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWKPDTLBb_xe",
        "outputId": "14d7de7a-9349-4495-dfbb-0591be773c38"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of mean vector = (35,)\n",
            "shape of std vector = (35,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_num = (features_num - mean) / std\n",
        "print(features_num.shape)\n",
        "print(features_num[0][0])\n",
        "\n",
        "features[:,:,:35] = features_num \n",
        "print(features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Uiykt1Le0Vb",
        "outputId": "01811515-81a6-45d4-f385-caee707da7e0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(21361, 22, 35)\n",
            "[-2.13402071 -1.37252412 -1.77469422 -1.23838184 -1.99655923 -2.73920252\n",
            " -2.04805005 -1.86919281 -1.97499507 -2.01346997 -0.99274355 -1.92213186\n",
            " -1.8076617  -2.24156397 -2.00765514 -1.48199584  0.46714265 -2.30508048\n",
            " -1.20626801 -2.30813745  0.28191452 -1.50510393 -0.48443304 -0.86434178\n",
            " -1.24425375 -0.70543806 -0.52144225 -1.16782289 -1.33347824 -0.76945225\n",
            "  2.46779935  1.85987951  0.88080786  1.93442016  2.57236977]\n",
            "(21361, 22, 66)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_num = np.round(features.shape[0]*0.7)\n",
        "val_num = np.round(features.shape[0]*0.15)\n",
        "test_num = np.round(features.shape[0]*0.15)\n",
        "train_index = int(train_num)\n",
        "val_index = int(train_num+val_num)\n",
        "test_index = int(val_index+test_num)\n",
        "\n",
        "print(f\"Training sample size = {train_num}, Validation set size = {val_num}, Test set size = {test_num}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj0zzDEXYudr",
        "outputId": "2ab55d8d-5aa5-41b9-b63f-bd23cfe1eb9e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training sample size = 14953.0, Validation set size = 3204.0, Test set size = 3204.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are working with time series data, i.e. each in the match dataset and thus our numpy array is in ordered by match date. "
      ],
      "metadata": {
        "id": "q3pALgFpl9k2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features = features[:train_index,:,:]\n",
        "train_labels = labels[:train_index,:]\n",
        "\n",
        "val_features = features[train_index:val_index,:,:]\n",
        "val_labels = labels[train_index:val_index,:]\n",
        "\n",
        "test_features = features[val_index:,:,:]\n",
        "test_labels = labels[val_index:,:]"
      ],
      "metadata": {
        "id": "-195LDKtblLi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_features.shape, test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naQ1hbtbhBb2",
        "outputId": "1e3b6c6b-2193-4619-8bae-3b1267224a90"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3204, 22, 66) (3204, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#turn data into dataset objects \n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_features, val_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels))"
      ],
      "metadata": {
        "id": "gXwXSMFMkk3e"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive Implementations\n",
        "\n",
        "We get a baseline model by implementing a naive dense model that just flattens all weights, and then we try a first stab at a transformer with 12 layers that self encodes all of the data and has a MLP classifier as a head\n",
        "\n"
      ],
      "metadata": {
        "id": "SGwIdSy3uLru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, now we finally have data that's ready to go in tf. let's run it through a very simple network and see what we get. "
      ],
      "metadata": {
        "id": "grU63LpDlrhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_dataset.shuffle(100).batch(32)\n",
        "val_ds = val_dataset.batch(32)\n",
        "test_ds = test_dataset.batch(32)"
      ],
      "metadata": {
        "id": "2NtVi8ejtY13"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "simple_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(3,activation=\"softmax\",)\n",
        "])\n",
        "\n",
        "simple_model.compile(optimizer='adam',\n",
        "              loss=['categorical_crossentropy'] ,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "simple_model.build(input_shape=(None,22,66))\n",
        "simple_model.summary()"
      ],
      "metadata": {
        "id": "4j0wWwaJlHcw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee06f6d2-d851-48d2-a55d-4229dc238a62"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 1452)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 4359      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,359\n",
            "Trainable params: 4,359\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simple_model.fit(train_ds, epochs=10)"
      ],
      "metadata": {
        "id": "bSYCK2fdrU6V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46a08170-91d5-4c06-f4a2-391ed4001cb2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "468/468 [==============================] - 5s 4ms/step - loss: 1.0809 - accuracy: 0.4767\n",
            "Epoch 2/10\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 1.0232 - accuracy: 0.5008\n",
            "Epoch 3/10\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 1.0066 - accuracy: 0.5158\n",
            "Epoch 4/10\n",
            "468/468 [==============================] - 2s 5ms/step - loss: 0.9972 - accuracy: 0.5212\n",
            "Epoch 5/10\n",
            "468/468 [==============================] - 1s 3ms/step - loss: 0.9961 - accuracy: 0.5250\n",
            "Epoch 6/10\n",
            "468/468 [==============================] - 1s 2ms/step - loss: 0.9861 - accuracy: 0.5272\n",
            "Epoch 7/10\n",
            "468/468 [==============================] - 1s 2ms/step - loss: 0.9818 - accuracy: 0.5312\n",
            "Epoch 8/10\n",
            "468/468 [==============================] - 1s 2ms/step - loss: 0.9846 - accuracy: 0.5292\n",
            "Epoch 9/10\n",
            "468/468 [==============================] - 1s 2ms/step - loss: 0.9766 - accuracy: 0.5346\n",
            "Epoch 10/10\n",
            "468/468 [==============================] - 1s 2ms/step - loss: 0.9725 - accuracy: 0.5356\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0c90217b90>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test acc: {simple_model.evaluate(test_ds)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KK5VsWC6tcO8",
        "outputId": "39a85ad2-a285-43a4-ca30-2c96e8666636"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101/101 [==============================] - 0s 2ms/step - loss: 1.0828 - accuracy: 0.4697\n",
            "Test acc: [1.0827760696411133, 0.4697253406047821]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer Implementation\n",
        "Here we are going to follow some more granular techniques to create a couple sub modules from Keras Layers to implement an architecture closely related to [Vision Transformers ](https://https://github.com/keras-team/keras-io/blob/master/examples/vision/image_classification_with_vision_transformer.py)\n",
        "\n",
        "So far we ran our data, (which is a (sample x 22 players x 66 features) vector through both a simple flatten --> MLP layer, resulting in ~45% test accuracy, and a naive transformer implementation where we put it through a linear projection layer, 12 layers of 4 head multihead self attention, global max pooling and then a classification layer on top of that. \n",
        "\n",
        "We will be changing our implementation to make it more customizable and now try two different approaches: \n",
        "1.) Separate the data to learn a representation of each team through self attention (i.e. two (11xfeature length) inputs) \n",
        "2.) Same approach as before \n",
        "\n",
        "In both cases we will increase numper of MLP layers, and also flatten the output of the self attention blocks into (batch x (256*11)) vectors as inputs into the MLP. \n",
        "\n",
        "We will also take the position indicator in the feature vector (second to last position), and embed it and add it to back to the feature vector before putting it through the attention block. This is the same approach taken in the Vision Transformers paper linked above. "
      ],
      "metadata": {
        "id": "YWUsC0r3ucQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_tensor(features,labels, index1=0,index2=-1):\n",
        "  '''Start with our full feature tensor of shape (matches x 22 x feature_size) \n",
        "  and slice it into train, val, and test set with \n",
        "  splitting of teams and then creating a separate tensor for \n",
        "  positions in each match \n",
        "  Output will be:\n",
        "   team1 (index1:index2 x 11 x feature_size-2)\n",
        "   team2 (index1:index2 x 11 x feature_size-2)\n",
        "   team1_pos (index1:index2 x 11 x 1)\n",
        "   team2_pos (index1:index2 x 11 x 1)\n",
        "   Slice the labels tensor to be (index1:index2, 3,) '''\n",
        "  \n",
        "  features_ = features[index1:index2,:,:-2]\n",
        "  team_1 = features_[:,:11,:]\n",
        "  team_2 = features_[:,11:,:]\n",
        "  team_1_pos = features[index1:index2,:11,-2:-1]\n",
        "  team_2_pos = features[index1:index2,11:,-2:-1]\n",
        "  labels_ = labels[index1:index2,:]\n",
        "\n",
        "  return team_1, team_2, team_1_pos, team_2_pos, labels_\n",
        "\n",
        "\n",
        "train_team_1, train_team_2, train_team_1_pos, train_team_2_pos,\\\n",
        " train_labels = split_tensor(features, labels, index2=train_index)\n",
        "\n",
        "val_team_1, val_team_2, val_team_1_pos, val_team_2_pos,\\\n",
        " val_labels = split_tensor(features,labels,index1=train_index,index2=val_index)\n",
        "\n",
        "\n",
        "test_team_1, test_team_2, test_team_1_pos, test_team_2_pos,\\\n",
        "test_labels = split_tensor(features, labels, index1=val_index)"
      ],
      "metadata": {
        "id": "QsiBLfDS0e1z"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_index, val_index, test_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyu1IFU0Qxkm",
        "outputId": "a21baab6-a3af-4043-8c6c-7ee1d25ee39f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14953 18157 21361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Team 1 Shape = {train_team_1.shape}, \\nTeam 2 Shape = {train_team_2.shape}\\\n",
        "        \\nTeam 1 Positions = {train_team_1_pos.shape}\")\n",
        "\n",
        "print(f\"Team 1 Shape = {val_team_1.shape}, \\nTeam 2 Shape = {val_team_2.shape}\\\n",
        "        \\nTeam 1 Positions = {val_team_1_pos.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Team 1 Shape = {test_team_1.shape}, \\nTeam 2 Shape = {test_team_2.shape}\\\n",
        "        \\nTeam 1 Positions = {test_team_1_pos.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt2VZWB6GOVW",
        "outputId": "9e71bbb7-9b52-444b-81f1-3872cb5eded8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Team 1 Shape = (14953, 11, 64), \n",
            "Team 2 Shape = (14953, 11, 64)        \n",
            "Team 1 Positions = (14953, 11, 1)\n",
            "Team 1 Shape = (3204, 11, 64), \n",
            "Team 2 Shape = (3204, 11, 64)        \n",
            "Team 1 Positions = (3204, 11, 1)\n",
            "Team 1 Shape = (3203, 11, 64), \n",
            "Team 2 Shape = (3203, 11, 64)        \n",
            "Team 1 Positions = (3203, 11, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(({\"input_1\":train_team_1,\"input_2\":train_team_2}, train_labels))\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(({\"input_1\":val_team_1,\"input_2\":val_team_2}, val_labels))\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(({\"input_1\":test_team_1,\"input_2\":test_team_2}, test_labels))\n"
      ],
      "metadata": {
        "id": "r8eeso6pF4hE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 3\n",
        "input_shape = (11,64)\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 32\n",
        "num_epochs = 100\n",
        "num_players = 11\n",
        "projection_dim = 256\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "] \n",
        "\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n"
      ],
      "metadata": {
        "id": "YVoR-R9sxYT7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Implement MLP layers'''\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Encoder(layers.Layer):\n",
        "  '''Takes the original input layer, projects it and then adds a positional encoding layer to it'''\n",
        "  def __init__(self, num_players, projection_dim):\n",
        "      super().__init__()\n",
        "      self.num_players = num_players\n",
        "      self.projection = layers.Dense(units=projection_dim)\n",
        "      self.position_embedding = layers.Embedding(\n",
        "          input_dim=num_players, output_dim=projection_dim\n",
        "      )\n",
        "\n",
        "  def call(self, players):\n",
        "      positions = tf.range(start=0, limit=self.num_players, delta=1)\n",
        "      encoded = self.projection(players) + self.position_embedding(positions)\n",
        "      return encoded\n",
        "  "
      ],
      "metadata": {
        "id": "3XT6J2z5TTg8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_block(team):\n",
        "    '''Input a particular team as a set of (batch_size, 11, feature size) vectors,\n",
        "     and returns output of transformer block that will be used in classification downstream'''\n",
        "    encoded_players = Encoder(num_players, projection_dim)(team)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_players)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_players])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x4 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "       \n",
        "        # Skip connection 2.\n",
        "        encoded_players1= layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_players)\n",
        "\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    return representation\n",
        "    "
      ],
      "metadata": {
        "id": "1wW_3ul3VfeQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input1 = layers.Input(shape=input_shape)\n",
        "input2 = layers.Input(shape=input_shape)\n",
        "\n",
        "representation1 = transformer_block(input1)\n",
        "representation2 = transformer_block(input2)\n",
        "\n",
        "representation = layers.Concatenate()([representation1,representation2])\n",
        "features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "# Classify outputs.\n",
        "logits = layers.Dense(num_classes)(features)\n",
        "# Create the Keras model.\n",
        "model = keras.Model(inputs=[input1, input2], outputs=logits)"
      ],
      "metadata": {
        "id": "04EtgKpLe__s"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[keras.metrics.CategoricalAccuracy(name=\"accuracy\")],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/content/drive/MyDrive/Data Mining/\"\n",
        "    checkpoint_list = [\n",
        "        keras.callbacks.ModelCheckpoint(\n",
        "          checkpoint_filepath,\n",
        "          monitor=\"val_accuracy\",\n",
        "          save_best_only=True,\n",
        "          save_weights_only=True,\n",
        "        ), \n",
        "        keras.callbacks.EarlyStopping(\n",
        "          monitor=\"val_accuracy\",\n",
        "          patience=2,\n",
        "        ),\n",
        "\n",
        "        keras.callbacks.TensorBoard(\n",
        "          log_dir=\"/content/drive/MyDrive/Data Mining/\",\n",
        "          ),\n",
        "       ]\n",
        "\n",
        "    history = model.fit(\n",
        "        x = [train_team_1, train_team_2],\n",
        "        y = train_labels,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        validation_data = ([val_team_1,val_team_2],val_labels),\n",
        "        callbacks=checkpoint_list,\n",
        "    )\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "X4nBRNQ_XvnX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = run_experiment(model)"
      ],
      "metadata": {
        "id": "hA1lMgV-hN76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = \"/content/drive/MyDrive/Data Mining/\"\n",
        "model.load_weights(checkpoint_filepath)\n",
        "loss, accuracy = model.evaluate([test_team_1, test_team_2],test_labels)\n",
        "print(f\"Test accuracy: {accuracy*100:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRp8IulhhnPf",
        "outputId": "1338b2f5-5e4f-4a16-d838-c32dead92801"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101/101 [==============================] - 0s 4ms/step - loss: 0.9924 - accuracy: 0.5301\n",
            "Test accuracy: 53.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir /checkpoint_filepath"
      ],
      "metadata": {
        "id": "Bl5H-1ZS_krR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill 553"
      ],
      "metadata": {
        "id": "A9AGRmzlCoEq"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ERjAgcMYC-pE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}