{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMrvRgx6Smeh28Ar8CovOUg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ncorriveau/transformers_for_prediction/blob/main/attention_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Processing \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ziA0RLzcuFEn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPtMTp3TXrBp",
        "outputId": "665645a0-078c-430e-c991-df6b600a85c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4pQ362TX_w4",
        "outputId": "bab2810e-8f58-47d4-d2f0-c12d5f15c48a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.18.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 14.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "from keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ],
      "metadata": {
        "id": "EIlpu7lkXzJF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load our numpy tensors to load into keras \n",
        "features_path = '/content/drive/MyDrive/Data Mining/features.npy'\n",
        "labels_path = '/content/drive/MyDrive/Data Mining/labels.npy'\n",
        "features = np.load(features_path)\n",
        "labels = np.load(labels_path)"
      ],
      "metadata": {
        "id": "0chvsS9zYQ5R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(features.shape)\n",
        "print(labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyzQOW2gYpcE",
        "outputId": "ae0b63fa-4dd2-4ace-ef4a-e6375223d838"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(21361, 22, 66)\n",
            "(21361, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize the numerical features which are locatead in cols 0-35\n",
        "features_num = features[:,:,:35]\n",
        "print(features_num.shape)\n",
        "\n",
        "features_num[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqWIojDFZeTE",
        "outputId": "638ef4aa-0674-4f70-e6d5-2067040b3142"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(21361, 22, 35)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([58., 67., 25., 25., 25., 25., 10., 25., 16., 12., 46., 35., 45.,\n",
              "       41., 41., 57., 72., 25., 56., 40., 73., 25., 57., 40., 33., 50.,\n",
              "       48., 25., 25., 35., 63., 51., 46., 53., 68.])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = np.mean(features_num,axis=(0,1))\n",
        "print(f\"shape of mean vector = {mean.shape}\")\n",
        "\n",
        "std = np.std(features_num,axis=(0,1))\n",
        "print(f\"shape of std vector = {std.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWKPDTLBb_xe",
        "outputId": "5677b732-9a2e-4c77-abb4-1c33001d83c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of mean vector = (35,)\n",
            "shape of std vector = (35,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_num = (features_num - mean) / std\n",
        "print(features_num.shape)\n",
        "print(features_num[0][0])\n",
        "\n",
        "features[:,:,:35] = features_num \n",
        "print(features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Uiykt1Le0Vb",
        "outputId": "b5390f1e-6efc-4acb-a615-fdfedf1620c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(21361, 22, 35)\n",
            "[-2.13402071 -1.37252412 -1.77469422 -1.23838184 -1.99655923 -2.73920252\n",
            " -2.04805005 -1.86919281 -1.97499507 -2.01346997 -0.99274355 -1.92213186\n",
            " -1.8076617  -2.24156397 -2.00765514 -1.48199584  0.46714265 -2.30508048\n",
            " -1.20626801 -2.30813745  0.28191452 -1.50510393 -0.48443304 -0.86434178\n",
            " -1.24425375 -0.70543806 -0.52144225 -1.16782289 -1.33347824 -0.76945225\n",
            "  2.46779935  1.85987951  0.88080786  1.93442016  2.57236977]\n",
            "(21361, 22, 66)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_num = np.round(features.shape[0]*0.7)\n",
        "val_num = np.round(features.shape[0]*0.15)\n",
        "test_num = np.round(features.shape[0]*0.15)\n",
        "train_index = int(train_num)\n",
        "val_index = int(train_num+val_num)\n",
        "test_index = int(val_index+test_num)\n",
        "\n",
        "print(f\"Training sample size = {train_num}, Validation set size = {val_num}, Test set size = {test_num}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj0zzDEXYudr",
        "outputId": "07b009c2-2750-4e08-bd8b-24105db65a80"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training sample size = 14953.0, Validation set size = 3204.0, Test set size = 3204.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are working with time series data, i.e. each in the match dataset and thus our numpy array is in ordered by match date. "
      ],
      "metadata": {
        "id": "q3pALgFpl9k2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features = features[:train_index,:,:]\n",
        "train_labels = labels[:train_index,:]\n",
        "\n",
        "val_features = features[train_index:val_index,:,:]\n",
        "val_labels = labels[train_index:val_index,:]\n",
        "\n",
        "test_features = features[val_index:,:,:]\n",
        "test_labels = labels[val_index:,:]"
      ],
      "metadata": {
        "id": "-195LDKtblLi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_features.shape, test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naQ1hbtbhBb2",
        "outputId": "c7291058-b28f-4029-ff93-f82890694c14"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3204, 22, 66) (3204, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert numpy array to float32 \n",
        "train_features, train_labels = np.float32(train_features), np.float32(train_labels)\n",
        "val_features, val_labels = np.float32(val_features), np.float32(val_labels)\n",
        "test_features, test_labels = np.float32(test_features), np.float32(test_labels)"
      ],
      "metadata": {
        "id": "_TKOei7rnLfv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#turn data into dataset objects \n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_features, val_labels))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels))"
      ],
      "metadata": {
        "id": "gXwXSMFMkk3e"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  x, y = next(iter(train_dataset)) \n",
        "  print(x.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFUmQHFvmKnV",
        "outputId": "3284e0f7-c813-4de8-ef3f-9e7f724d13bf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<dtype: 'float32'>\n",
            "<dtype: 'float32'>\n",
            "<dtype: 'float32'>\n",
            "<dtype: 'float32'>\n",
            "<dtype: 'float32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive Implementations\n",
        "\n",
        "We get a baseline model by implementing a naive dense model that just flattens all weights, and then we try a first stab at a transformer with 12 layers that self encodes all of the data and has a MLP classifier as a head\n",
        "\n"
      ],
      "metadata": {
        "id": "SGwIdSy3uLru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, now we finally have data that's ready to go in tf. let's run it through a very simple network and see what we get. "
      ],
      "metadata": {
        "id": "grU63LpDlrhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_dataset.shuffle(100).batch(32)\n",
        "val_ds = val_dataset.batch(32)\n",
        "test_ds = test_dataset.batch(32)"
      ],
      "metadata": {
        "id": "2NtVi8ejtY13"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "simple_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(3,activation=\"softmax\",)\n",
        "])\n",
        "\n",
        "simple_model.compile(optimizer='adam',\n",
        "              loss=['categorical_crossentropy'] ,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "simple_model.build(input_shape=(None,22,66))\n",
        "simple_model.summary()"
      ],
      "metadata": {
        "id": "4j0wWwaJlHcw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee06f6d2-d851-48d2-a55d-4229dc238a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 1452)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 4359      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,359\n",
            "Trainable params: 4,359\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simple_model.fit(train_ds, epochs=10)"
      ],
      "metadata": {
        "id": "bSYCK2fdrU6V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46a08170-91d5-4c06-f4a2-391ed4001cb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "468/468 [==============================] - 5s 4ms/step - loss: 1.0809 - accuracy: 0.4767\n",
            "Epoch 2/10\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 1.0232 - accuracy: 0.5008\n",
            "Epoch 3/10\n",
            "468/468 [==============================] - 2s 4ms/step - loss: 1.0066 - accuracy: 0.5158\n",
            "Epoch 4/10\n",
            "468/468 [==============================] - 2s 5ms/step - loss: 0.9972 - accuracy: 0.5212\n",
            "Epoch 5/10\n",
            "468/468 [==============================] - 1s 3ms/step - loss: 0.9961 - accuracy: 0.5250\n",
            "Epoch 6/10\n",
            "468/468 [==============================] - 1s 2ms/step - loss: 0.9861 - accuracy: 0.5272\n",
            "Epoch 7/10\n",
            "468/468 [==============================] - 1s 2ms/step - loss: 0.9818 - accuracy: 0.5312\n",
            "Epoch 8/10\n",
            "468/468 [==============================] - 1s 2ms/step - loss: 0.9846 - accuracy: 0.5292\n",
            "Epoch 9/10\n",
            "468/468 [==============================] - 1s 2ms/step - loss: 0.9766 - accuracy: 0.5346\n",
            "Epoch 10/10\n",
            "468/468 [==============================] - 1s 2ms/step - loss: 0.9725 - accuracy: 0.5356\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0c90217b90>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test acc: {simple_model.evaluate(test_ds)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KK5VsWC6tcO8",
        "outputId": "39a85ad2-a285-43a4-ca30-2c96e8666636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101/101 [==============================] - 0s 2ms/step - loss: 1.0828 - accuracy: 0.4697\n",
            "Test acc: [1.0827760696411133, 0.4697253406047821]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also try implementing a classic algorithm for table based data like a xgb tree"
      ],
      "metadata": {
        "id": "C4ss1lJUIzjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer Implementation\n",
        "Here we are going to follow some more granular techniques to create a couple sub modules from Keras Layers to implement an architecture closely related to [Vision Transformers ](https://https://github.com/keras-team/keras-io/blob/master/examples/vision/image_classification_with_vision_transformer.py)\n",
        "\n",
        "So far we ran our data, (which is a (sample x 22 players x 66 features) vector through both a simple flatten --> MLP layer, resulting in ~45% test accuracy, and a naive transformer implementation where we put it through a linear projection layer, 12 layers of 4 head multihead self attention, global max pooling and then a classification layer on top of that. \n",
        "\n",
        "We will be changing our implementation to make it more customizable and now try two different approaches: \n",
        "1.) Separate the data to learn a representation of each team through self attention (i.e. two (11xfeature length) inputs) \n",
        "2.) Same approach as before \n",
        "\n",
        "In both cases we will increase numper of MLP layers, and also flatten the output of the self attention blocks into (batch x (256*11)) vectors as inputs into the MLP. \n",
        "\n",
        "We will also take the position indicator in the feature vector (second to last position), and embed it and add it to back to the feature vector before putting it through the attention block. This is the same approach taken in the Vision Transformers paper linked above. "
      ],
      "metadata": {
        "id": "YWUsC0r3ucQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up model"
      ],
      "metadata": {
        "id": "AhsPHnF6Gui2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_tensor(features,labels, index1=0,index2=-1):\n",
        "  '''Start with our full feature tensor of shape (matches x 22 x feature_size) \n",
        "  and slice it into train, val, and test set with \n",
        "  splitting of teams and then creating a separate tensor for \n",
        "  positions in each match \n",
        "  Output will be:\n",
        "   team1 (index1:index2 x 11 x feature_size-2)\n",
        "   team2 (index1:index2 x 11 x feature_size-2)\n",
        "   team1_pos (index1:index2 x 11 x 1)\n",
        "   team2_pos (index1:index2 x 11 x 1)\n",
        "   Slice the labels tensor to be (index1:index2, 3,) '''\n",
        "  \n",
        "  features_ = features[index1:index2,:,:-2]\n",
        "  team_1 = features_[:,:11,:]\n",
        "  team_2 = features_[:,11:,:]\n",
        "  team_1_pos = features[index1:index2,:11,-2:-1]\n",
        "  team_2_pos = features[index1:index2,11:,-2:-1]\n",
        "  labels_ = labels[index1:index2,:]\n",
        "\n",
        "  return team_1, team_2, team_1_pos, team_2_pos, labels_\n",
        "\n",
        "\n",
        "train_team_1, train_team_2, train_team_1_pos, train_team_2_pos,\\\n",
        " train_labels = split_tensor(features, labels, index2=train_index)\n",
        "\n",
        "val_team_1, val_team_2, val_team_1_pos, val_team_2_pos,\\\n",
        " val_labels = split_tensor(features,labels,index1=train_index,index2=val_index)\n",
        "\n",
        "\n",
        "test_team_1, test_team_2, test_team_1_pos, test_team_2_pos,\\\n",
        "test_labels = split_tensor(features, labels, index1=val_index)"
      ],
      "metadata": {
        "id": "QsiBLfDS0e1z"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_index, val_index, test_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyu1IFU0Qxkm",
        "outputId": "e61ae4a7-a5e7-4d9a-cb17-7c4429f8de0d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14953 18157 21361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Team 1 Shape = {train_team_1.shape}, \\nTeam 2 Shape = {train_team_2.shape}\\\n",
        "        \\nTeam 1 Positions = {train_team_1_pos.shape}\")\n",
        "\n",
        "print(f\"Team 1 Shape = {val_team_1.shape}, \\nTeam 2 Shape = {val_team_2.shape}\\\n",
        "        \\nTeam 1 Positions = {val_team_1_pos.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Team 1 Shape = {test_team_1.shape}, \\nTeam 2 Shape = {test_team_2.shape}\\\n",
        "        \\nTeam 1 Positions = {test_team_1_pos.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt2VZWB6GOVW",
        "outputId": "9f23b9df-a558-44a5-f359-31c640981864"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Team 1 Shape = (14953, 11, 64), \n",
            "Team 2 Shape = (14953, 11, 64)        \n",
            "Team 1 Positions = (14953, 11, 1)\n",
            "Team 1 Shape = (3204, 11, 64), \n",
            "Team 2 Shape = (3204, 11, 64)        \n",
            "Team 1 Positions = (3204, 11, 1)\n",
            "Team 1 Shape = (3203, 11, 64), \n",
            "Team 2 Shape = (3203, 11, 64)        \n",
            "Team 1 Positions = (3203, 11, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(({\"input_1\":train_team_1,\"input_2\":train_team_2}, train_labels))\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(({\"input_1\":val_team_1,\"input_2\":val_team_2}, val_labels))\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(({\"input_1\":test_team_1,\"input_2\":test_team_2}, test_labels))\n"
      ],
      "metadata": {
        "id": "r8eeso6pF4hE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 3\n",
        "input_shape = (11,64)\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 32\n",
        "num_epochs = 100\n",
        "num_players = 11\n",
        "projection_dim = 256\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "] \n",
        "\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n"
      ],
      "metadata": {
        "id": "YVoR-R9sxYT7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "  '''Implement MLP layers'''\n",
        "  for units in hidden_units:\n",
        "      x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "      x = layers.Dropout(dropout_rate)(x)\n",
        "  return x\n",
        "\n",
        "class Encoder(layers.Layer):\n",
        "  '''Takes the original input layer, projects it and then adds a positional encoding layer to it'''\n",
        "  def __init__(self, num_players, projection_dim):\n",
        "      super().__init__()\n",
        "      self.num_players = num_players\n",
        "      self.projection = layers.Dense(units=projection_dim)\n",
        "      self.position_embedding = layers.Embedding(\n",
        "          input_dim=num_players, output_dim=projection_dim\n",
        "      )\n",
        "\n",
        "  def call(self, players):\n",
        "      positions = tf.range(start=0, limit=self.num_players, delta=1)\n",
        "      encoded = self.projection(players) + self.position_embedding(positions)\n",
        "      return encoded\n",
        "  "
      ],
      "metadata": {
        "id": "3XT6J2z5TTg8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_block(team,projection_dim):\n",
        "    '''Input a particular team as a set of (batch_size, 11, feature size) vectors,\n",
        "     and returns output of transformer block that will be used in classification downstream'''\n",
        "    encoded_players = Encoder(num_players, projection_dim)(team)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_players)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_players])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x4 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "       \n",
        "        # Skip connection 2.\n",
        "        encoded_players= layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_players)\n",
        "\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    return representation\n",
        "    "
      ],
      "metadata": {
        "id": "1wW_3ul3VfeQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define model\n",
        "input1 = layers.Input(shape=input_shape)\n",
        "input2 = layers.Input(shape=input_shape)\n",
        "\n",
        "representation1 = transformer_block(input1, projection_dim)\n",
        "representation2 = transformer_block(input2, projection_dim)\n",
        "\n",
        "representation = layers.Concatenate()([representation1,representation2])\n",
        "features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "# Classify outputs.\n",
        "logits = layers.Dense(num_classes)(features)\n",
        "# Create the Keras model.\n",
        "model = keras.Model(inputs=[input1,input2], outputs=logits)"
      ],
      "metadata": {
        "id": "04EtgKpLe__s"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Run Model Training"
      ],
      "metadata": {
        "id": "KrSy7MuYG1Pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[keras.metrics.CategoricalAccuracy(name=\"accuracy\")],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/content/drive/MyDrive/Data Mining/\"\n",
        "    checkpoint_list = [\n",
        "        keras.callbacks.ModelCheckpoint(\n",
        "          checkpoint_filepath,\n",
        "          monitor=\"val_accuracy\",\n",
        "          save_best_only=True,\n",
        "          save_weights_only=True,\n",
        "        ), \n",
        "        keras.callbacks.EarlyStopping(\n",
        "          monitor=\"val_accuracy\",\n",
        "          patience=2,\n",
        "        ),\n",
        "        keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n",
        "        ),\n",
        "        keras.callbacks.TensorBoard(\n",
        "          log_dir=\"/content/drive/MyDrive/Data Mining/\",\n",
        "          ),\n",
        "       ]\n",
        "\n",
        "    history = model.fit(\n",
        "        x = [train_team_1, train_team_2],\n",
        "        y = train_labels,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_data = ([val_team_1,val_team_2],val_labels),\n",
        "        callbacks=checkpoint_list,\n",
        "    )\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "X4nBRNQ_XvnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = run_experiment(model)"
      ],
      "metadata": {
        "id": "hA1lMgV-hN76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = \"/content/drive/MyDrive/Data Mining/\"\n",
        "model.load_weights(checkpoint_filepath)\n",
        "loss, accuracy = model.evaluate([test_team_1, test_team_2],test_labels)\n",
        "print(f\"Test accuracy: {accuracy*100:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRp8IulhhnPf",
        "outputId": "1338b2f5-5e4f-4a16-d838-c32dead92801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101/101 [==============================] - 0s 4ms/step - loss: 0.9924 - accuracy: 0.5301\n",
            "Test accuracy: 53.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir /checkpoint_filepath"
      ],
      "metadata": {
        "id": "Bl5H-1ZS_krR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill 553"
      ],
      "metadata": {
        "id": "A9AGRmzlCoEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot learning curves \n",
        "metric = \"accuracy\"\n",
        "plt.figure()\n",
        "plt.plot(history.history[metric])\n",
        "plt.plot(history.history[\"val_\" + metric])\n",
        "plt.title(\"model \" + metric)\n",
        "plt.ylabel(metric, fontsize=\"large\")\n",
        "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
        "plt.legend([\"train\", \"val\"], loc=\"best\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "HN6RVpeqXj6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model Optimization"
      ],
      "metadata": {
        "id": "6kTk9UHDP0H8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimize model hyper-parameters using Keras Tuner"
      ],
      "metadata": {
        "id": "u3FOXpU0P4zM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import keras_tuner as kt\n",
        "except:\n",
        "  !pip install -q -U keras-tuner\n",
        "  import keras_tuner as kt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQTxFoOHvjnc",
        "outputId": "83710a50-ef4b-40ce-ad9e-e889445a2def"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 135 kB 15.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 64.4 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define build model function \n",
        "#following outline on https://keras.io/guides/keras_tuner/custom_tuner/\n",
        "class MyHyperModel(kt.HyperModel):\n",
        "\n",
        "    def build(self, hp):\n",
        "        \"\"\"Build transformer based classifier\"\"\"\n",
        "        #define hyperparameters\n",
        "        projection_dim=hp.Int(\"projection_dim\", 128, 512, step=128, default=256)\n",
        "        dropout_rate=hp.Float(\"dropout_rate\",0.1,0.6)\n",
        "\n",
        "        #####define model#####\n",
        "\n",
        "        #set inputs for two teams \n",
        "        input1 = layers.Input(shape=(11,64))\n",
        "        input2 = layers.Input(shape=(11,64))\n",
        "        \n",
        "        #get transformation team representations \n",
        "        representation1 = transformer_block(input1, projection_dim)\n",
        "        representation2 = transformer_block(input2, projection_dim)\n",
        "\n",
        "        #concat representations \n",
        "        representation = layers.Concatenate()([representation1,\n",
        "                                               representation2])\n",
        "        # Classify outputs in to W, T, L\n",
        "        output = layers.Dense(3)(representation)\n",
        "        # Create the Keras model.\n",
        "        model = keras.Model(inputs=[input1, input2], outputs=output)\n",
        "\n",
        "        #define optimizer\n",
        "        optimizer = keras.optimizers.Adam(\n",
        "            hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\", default=1e-3)\n",
        "          )\n",
        "        \n",
        "        #define loss_fn\n",
        "        loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "        #compile model\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=loss_fn,\n",
        "            metrics=[\"accuracy\"],\n",
        "        )\n",
        "\n",
        "        return model\n"
      ],
      "metadata": {
        "id": "LGqFj4zhkIqL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define tuner \n",
        "tuner = kt.BayesianOptimization(\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=100,\n",
        "    executions_per_trial=2,\n",
        "    hypermodel= MyHyperModel(),\n",
        "    directory=\"/content/drive/MyDrive/Data Mining/\",\n",
        "    project_name=\"model_optimization\",\n",
        "    overwrite=True,\n",
        "  ) "
      ],
      "metadata": {
        "id": "EdsT9oWDu4Gd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search_space_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiRRdDmBihB4",
        "outputId": "430674ef-a68e-4e91-a719-bf228cb208fa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 3\n",
            "projection_dim (Int)\n",
            "{'default': 256, 'conditions': [], 'min_value': 128, 'max_value': 512, 'step': 128, 'sampling': None}\n",
            "dropout_rate (Float)\n",
            "{'default': 0.1, 'conditions': [], 'min_value': 0.1, 'max_value': 0.6, 'step': None, 'sampling': None}\n",
            "learning_rate (Float)\n",
            "{'default': 0.001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#set callbacks \n",
        "callbacks = [\n",
        "  keras.callbacks.ModelCheckpoint(\n",
        "    filepath='/content/drive/MyDrive/Data Mining/',\n",
        "    monitor=\"val_accuracy\",\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "  ), \n",
        "  keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_accuracy\",\n",
        "    patience=5,\n",
        "  )]\n",
        "\n",
        "#start training \n",
        "tuner.search(\n",
        "    x = [train_team_1, train_team_2],\n",
        "    y = train_labels,\n",
        "    batch_size=batch_size,\n",
        "    epochs=100,\n",
        "    validation_data = ([val_team_1,val_team_2], val_labels),\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "#pull best 5 best hp's \n",
        "best_hps = tuner.get_best_hyperparameters(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev1XNaifjaG6",
        "outputId": "a7ec78f1-5d1f-4a0d-8cc0-7df8123a1ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 19 Complete [00h 10m 21s]\n",
            "val_accuracy: 0.5422908961772919\n",
            "\n",
            "Best val_accuracy So Far: 0.5435393452644348\n",
            "Total elapsed time: 03h 27m 30s\n",
            "\n",
            "Search: Running Trial #20\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "128               |384               |projection_dim\n",
            "0.21753           |0.19227           |dropout_rate\n",
            "0.0001            |0.0001            |learning_rate\n",
            "\n",
            "Epoch 1/100\n",
            "468/468 [==============================] - 45s 63ms/step - loss: 1.5512 - accuracy: 0.4209 - val_loss: 1.0670 - val_accuracy: 0.4654\n",
            "Epoch 2/100\n",
            "468/468 [==============================] - 28s 60ms/step - loss: 1.3027 - accuracy: 0.4411 - val_loss: 0.9971 - val_accuracy: 0.5247\n",
            "Epoch 3/100\n",
            "468/468 [==============================] - 27s 58ms/step - loss: 1.1505 - accuracy: 0.4552 - val_loss: 0.9739 - val_accuracy: 0.5393\n",
            "Epoch 4/100\n",
            "468/468 [==============================] - 30s 64ms/step - loss: 1.0589 - accuracy: 0.4830 - val_loss: 0.9673 - val_accuracy: 0.5396\n",
            "Epoch 5/100\n",
            "468/468 [==============================] - 28s 59ms/step - loss: 1.0244 - accuracy: 0.4998 - val_loss: 0.9821 - val_accuracy: 0.5331\n",
            "Epoch 6/100\n",
            "468/468 [==============================] - 27s 59ms/step - loss: 1.0112 - accuracy: 0.5037 - val_loss: 0.9754 - val_accuracy: 0.5337\n",
            "Epoch 7/100\n",
            "468/468 [==============================] - 26s 56ms/step - loss: 1.0030 - accuracy: 0.5086 - val_loss: 0.9892 - val_accuracy: 0.5278\n",
            "Epoch 8/100\n",
            "468/468 [==============================] - 27s 58ms/step - loss: 1.0008 - accuracy: 0.5109 - val_loss: 0.9718 - val_accuracy: 0.5387\n",
            "Epoch 9/100\n",
            "468/468 [==============================] - 27s 58ms/step - loss: 0.9944 - accuracy: 0.5155 - val_loss: 0.9712 - val_accuracy: 0.5353\n",
            "Epoch 1/100\n",
            "468/468 [==============================] - 45s 60ms/step - loss: 1.5318 - accuracy: 0.4226 - val_loss: 1.0376 - val_accuracy: 0.5025\n",
            "Epoch 2/100\n",
            "468/468 [==============================] - 28s 60ms/step - loss: 1.3178 - accuracy: 0.4358 - val_loss: 1.0106 - val_accuracy: 0.5331\n",
            "Epoch 3/100\n",
            "468/468 [==============================] - 28s 60ms/step - loss: 1.1743 - accuracy: 0.4547 - val_loss: 0.9794 - val_accuracy: 0.5228\n",
            "Epoch 4/100\n",
            "468/468 [==============================] - 28s 59ms/step - loss: 1.0748 - accuracy: 0.4757 - val_loss: 0.9761 - val_accuracy: 0.5325\n",
            "Epoch 5/100\n",
            "468/468 [==============================] - 28s 60ms/step - loss: 1.0238 - accuracy: 0.4966 - val_loss: 0.9698 - val_accuracy: 0.5393\n",
            "Epoch 6/100\n",
            "468/468 [==============================] - 29s 62ms/step - loss: 1.0122 - accuracy: 0.5076 - val_loss: 0.9806 - val_accuracy: 0.5247\n",
            "Epoch 7/100\n",
            "468/468 [==============================] - 30s 64ms/step - loss: 1.0044 - accuracy: 0.5085 - val_loss: 0.9784 - val_accuracy: 0.5284\n",
            "Epoch 8/100\n",
            "468/468 [==============================] - 30s 63ms/step - loss: 0.9991 - accuracy: 0.5123 - val_loss: 0.9723 - val_accuracy: 0.5443\n",
            "Epoch 9/100\n",
            "176/468 [==========>...................] - ETA: 16s - loss: 0.9932 - accuracy: 0.5119"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Further Experiments"
      ],
      "metadata": {
        "id": "dFnW6XKvP5e6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qNEt7HDxP8FU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}